{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math, copy\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split # used to split data set into (x, y) train and (x, y) test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])\ny_train = np.array([250, 300, 480,  430,   630, 730])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"text-align:center; background-color:#dc3545; padding:20px;\">\n  <h1 style=\"font-size:36px; color:#ffeeba;\"><b>Linear Regression From Scratch</b></h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Computing Cost\nThe term 'cost' in this assignment might be a little confusing since the data is housing cost. Here, cost is a measure how well our model is predicting the target price of the house. The term 'price' is used for housing data.\n\nThe equation for cost with one variable is:\n  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$ \n \nwhere \n  $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$\n  \n- $f_{w,b}(x^{(i)})$ is our prediction for example $i$ using parameters $w,b$.  \n- $(f_{w,b}(x^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction, and it's squared to ignore the negative values.\n","metadata":{}},{"cell_type":"code","source":"def cost_function(x_tr, y_tr, w, b):\n    m = len(x_tr)\n    cost = 0\n    \n    for i in range(m):\n        f_wb = w * x_tr[i] + b\n        cost += (f_wb - y_tr[i]) ** 2\n    \n    total_cost = (1 / (2 * m)) * cost\n    return total_cost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Gradient Descent\nThe term 'Gradient Descent' used to minimized any function in math, and in our problem we use it to get the minimum cost function.\n\nGradient Descent:$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n\\end{align*}$$\nwhere, parameters $w$, $b$ are updated simultaneously.  \nThe gradient is defined as:\n$$\n\\begin{align}\n\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n\\end{align}\n$$\n","metadata":{}},{"cell_type":"code","source":"def gradient_derivative(x_tr, y_tr, w, b):\n    d_dw = 0\n    d_db = 0\n    m =len(x_tr)\n    \n    for i in range(m):\n        f_wb = w * x_tr[i] + b\n        d_dw_i = (f_wb - y_tr[i]) * x_tr[i]\n        d_db_i = (f_wb - y_tr[i])\n        d_dw += d_dw_i\n        d_db += d_db_i\n    d_dw = d_dw / m\n    d_db = d_db /m\n    \n    return d_dw, d_db","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_descent(x_tr, y_tr, w_input, b_input, alpha, iterats, cost_function, gradient_derivative):\n    \n    j_history = []\n    w_b_history = []\n    w = w_input\n    b = b_input\n    \n    for i in range(iterats):\n        d_dw, d_db = gradient_derivative(x_tr, y_tr, w, b)\n        \n        w = w - alpha * d_dw\n        b = b -alpha * d_db\n        \n        if i<10000:\n            j_history.append(cost_function(x_tr, y_tr , w , b))\n            w_b_history.append([w,b])\n        \n        # Print cost every at intervals 10 times or as many iterations if < 10\n        if i % 1000 == 0:\n            print(f\"Iteration {i:4}: Cost {j_history[-1]:0.2e} \",\n                  f\"d_dw: {d_dw: 0.3e}, d_db: {d_db: 0.3e}  \",\n                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n        \n       \n    return w, b, j_history, w_b_history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize parameters\nw_init = 0\nb_init = 0\niterations = 10000\ntmp_alpha = 1.0e-2\n\n# run gradient descent\nw_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n                                                    iterations, cost_function, gradient_derivative)\nprint(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predictions","metadata":{}},{"cell_type":"code","source":"def compute_model_fucntion(x_tr, w, b):\n    m = x_tr.shape[0]\n    prediction = np.zeros(m)\n    for i in range(m):\n        prediction[i] = w * x_tr[i] + b\n    return prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = compute_model_fucntion(x_train, w_final, b_final)\nprediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ploting our Predictions","metadata":{}},{"cell_type":"code","source":"plt.scatter(\n    x = x_train,\n    y = y_train,\n    color='red',\n    marker='.',\n    label = 'Actual Values'\n)\n\nplt.plot(\n    x_train,\n    prediction,\n    color = 'blue',\n    label = 'Predict values'\n)\n\n\nplt.title('Houses prices')\nplt.xlabel('House size')\nplt.ylabel('House price')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# value of w\nprint(f'The value of w is: {w_final}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#value of b\nprint(f'The value of b is: {b_final}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"text-align:center; background-color:#ff99cc; padding:20px;\">\n  <h1 style=\"font-size:36px; color:#FFEBEB;\"><b>Linear Regression Using Sklearn</b></h1>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"### Convert numpy array to pandas Data Frame","metadata":{}},{"cell_type":"code","source":"train = pd.DataFrame(x_train)\ntest= pd.DataFrame(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split and Fit the model","metadata":{}},{"cell_type":"code","source":"lin = LinearRegression()\nx_train, x_test, y_train, y_test = train_test_split(train, test, test_size=0.2,random_state=0)\nlin = LinearRegression()\nlin.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = lin.predict(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the Predictions","metadata":{}},{"cell_type":"code","source":"plt.scatter(\n    x = x_train,\n    y = y_train,\n    color='red',\n    marker='.',\n    label = 'Actual Values'\n)\n\nplt.plot(\n    x_train,\n    prediction,\n    color = 'blue',\n    label = 'Predict values'\n)\n\n\nplt.title('Houses prices')\nplt.xlabel('House size')\nplt.ylabel('House price')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w = lin.coef_[0]\nprint(f'The value of w is: {w}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = lin.intercept_\nprint(f'The value of b is: {b}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}